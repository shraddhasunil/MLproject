{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DF=pd.read_csv('train.csv',index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAN occurence in Data\n",
    "#Question2 as 'NAN' in it\n",
    "\n",
    "#total number of NAN in each Column\n",
    "#train_DF.isnull().sum()\n",
    "\n",
    "#Index which have NAN Values\n",
    "#train_DF.isnull().any(1).nonzero()\n",
    "#train_DF.iloc[[105780, 201841]]\n",
    "\n",
    "#Drop The NAN Values\n",
    "train_DF.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note\n",
    "#No duplicates in ID\n",
    "\n",
    "#train_DF['id_duplicate']=train_DF.duplicated(['id'])\n",
    "#train_DF[train_DF['id_duplicate']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note\n",
    "#duplicates exist in qid1\n",
    "#qid is the question ID for question 1\n",
    "\n",
    "#train_DF['qid1_duplicate']=train_DF.duplicated(['qid1'])\n",
    "#train_DF[train_DF['qid1_duplicate']==True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note\n",
    "#duplicates exist in qid2\n",
    "#qid is the question ID for question 2\n",
    "\n",
    "#train_DF['qid2_duplicate']=train_DF.duplicated(['qid2'])\n",
    "#train_DF[train_DF['qid2_duplicate']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note\n",
    "# Question can be duplicates between Question1 and question 2\n",
    "#same question can come up as Question 1 in one example and Question 2 in other\n",
    "#Total Number of Unique Questions:347345\n",
    "\n",
    "\n",
    "#qid1=train_DF['qid1']\n",
    "#qid2=train_DF['qid2']\n",
    "#qid=qid1+qid2\n",
    "\n",
    "#Total Number of Unique Questions\n",
    "#print(len(qid.unique()))\n",
    "\n",
    "#Total Number of unique Questions in Question 1 + Total Number of Unique Questions in Question 2\n",
    "#print(len(qid2.unique())+len(qid1.unique()))\n",
    "\n",
    "#Maximum times a question repeated in data:6\n",
    "#qid.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Text Features\n",
    "- Number of characters\n",
    "- Number of words\n",
    "- 'End' Character in Questions(?vs.)\n",
    "- TFIDf and LSA(30 dimension)\n",
    "- LDA (check performance- 20 dimensions)\n",
    "- Question type- What, When , How, Why( more types have to be added)\n",
    "- Number of Capital Letters, Special Characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DF['question1'] = train_DF['question1'].astype(str)\n",
    "train_DF['question2'] = train_DF['question2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Character Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_count(a,b):\n",
    "    return len(a), len(b)\n",
    "\n",
    "train_DF['character_count_in_question1'],train_DF['character_count_in_question2']= zip(*train_DF.apply(lambda row: \n",
    "                                                                                                       character_count(row['question1'], row['question2']), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(a,b):\n",
    "    return len(nltk.tokenize.word_tokenize(a)), len(nltk.tokenize.word_tokenize(b))\n",
    "\n",
    "train_DF['word_count_in_question1'],train_DF['word_count_in_question2']= zip(*train_DF.apply(lambda row: \n",
    "                                                                                                       word_count(row['question1'], row['question2']), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Difference in Word and Character Counts\n",
    "\n",
    "#count as feature\n",
    "#Approaches:\n",
    "#count_difference<range => 0\n",
    "\n",
    "#train_DF['difference_in_word_count']=abs(train_DF['word_count_in_question1']-train_DF['word_count_in_question2'])\n",
    "#train_DF['difference_in_char_count']=abs(train_DF['character_count_in_question1']-train_DF['character_count_in_question2'])\n",
    "\n",
    "#train_DF_diplicates=train_DF[train_DF['is_duplicate']==1]\n",
    "\n",
    "#difference_in_word_count\n",
    "#label less than 5 has 1\n",
    "#train_DF_diplicates['difference_in_word_count'].value_counts()\n",
    "#train_DF['difference_in_word_count'].value_counts()\n",
    "\n",
    "#difference_in_character_count\n",
    "#Label less than 20 as 1\n",
    "#train_DF_diplicates['difference_in_char_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last Character Check\n",
    "def last_character(a,b):\n",
    "    if a[-1]=='?':\n",
    "        if b[-1]=='?':\n",
    "            return 1,1\n",
    "        else:\n",
    "            return 1,0\n",
    "    else:\n",
    "        if b[-1]=='?':\n",
    "            return 0,1\n",
    "        else:\n",
    "            return 0,0\n",
    "\n",
    "train_DF['Last_Character_in_question1'],train_DF['last_Character_in_question2']= zip(*train_DF.apply(lambda row: last_character(row['question1'], row['question2']), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer.fit(train_DF['question1']+train_DF['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "f = open('tfidf_vectorizer.save', 'wb')\n",
    "cPickle.dump(tfidf_vectorizer, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_question_1 = tfidf_vectorizer.transform(train_DF['question1'])\n",
    "tfidf_question_2 = tfidf_vectorizer.transform(train_DF['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  Actual number of tfidf features: %d\" % tfidf_question_1.get_shape()[1])\n",
    "print(\"  Actual number of tfidf features: %d\" % tfidf_question_2.get_shape()[1])\n",
    "print(\"\\nPerforming dimensionality reduction using LSA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "svd = TruncatedSVD(50)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "lsa.fit(tfidf_question_1+tfidf_question_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('lsa.save', 'wb')\n",
    "cPickle.dump(lsa, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_question1 = lsa.transform(tfidf_question_1)\n",
    "lsa_question2 = lsa.transform(tfidf_question_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "for string in train_DF['question1']+train_DF['question2']:\n",
    "    texts.append(nltk.tokenize.word_tokenize(string))\n",
    "    \n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, chunksize=1000,num_topics=25, id2word = dictionary, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('lda.save', 'wb')\n",
    "cPickle.dump(ldamodel, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.save('lda.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA with Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "preprocessed_texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in train_DF['question1']+train_DF['question2']:\n",
    "\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    preprocessed_texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "preprocessed_dictionary = corpora.Dictionary(preprocessed_texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "preprocessed_corpus = [preprocessed_dictionary.doc2bow(text) for text in preprocessed_texts]\n",
    "\n",
    "# generate LDA model\n",
    "preprocessed_ldamodel = gensim.models.ldamodel.LdaModel(preprocessed_corpus, chunksize=1000, num_topics=50, id2word = preprocessed_dictionary, passes=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('preprocessed_lda.save', 'wb')\n",
    "cPickle.dump(preprocessed_ldamodel, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_ldamodel.save('preprocessed_lda.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
