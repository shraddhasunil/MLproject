{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Siamese Architecture_Simple_LSTM\n",
    "#keras_1.1.2_theano_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "with open(os.path.expanduser('~')+'/.keras/keras.json','w') as f:\n",
    "    new_settings = \"\"\"{\\r\\n\n",
    "    \"epsilon\": 1e-07,\\r\\n\n",
    "    \"image_data_format\": \"channels_last\",\\n\n",
    "    \"backend\": \"theano\",\\r\\n\n",
    "    \"floatx\": \"float32\"\\r\\n\n",
    "    }\"\"\"\n",
    "    f.write(new_settings)\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunil\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Activation,Input, Flatten\n",
    "from keras.layers import Conv1D, Conv2D,GlobalMaxPooling2D,GlobalMaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "import numpy.random as rng\n",
    "import numpy as np\n",
    "import os\n",
    "import dill as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WordVectors\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DF=pd.read_csv('C:\\\\Users\\\\sunil\\\\Downloads\\\\train.csv',index_col='id')\n",
    "train_DF.reset_index(drop=True,inplace=True)\n",
    "train_DF['question1'] = train_DF['question1'].astype(str)\n",
    "train_DF['question2'] = train_DF['question2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for the Model\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "MAX_NB_WORDS = 40000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "numbers=re.compile(r'(\\d+\\.\\d+|\\d+)')\n",
    "def cleanText(text):\n",
    "    return numbers.sub('N',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "questions1=[]\n",
    "questions2=[]\n",
    "duplicates=[]\n",
    "\n",
    "for idx in range(train_DF.question1.shape[0]):\n",
    "    text=train_DF.question1[idx]\n",
    "    text=cleanText(text)\n",
    "    texts.append(text)\n",
    "    questions1.append(text)\n",
    "    \n",
    "    text=train_DF.question2[idx]\n",
    "    text=cleanText(text)\n",
    "    texts.append(text)\n",
    "    questions2.append(text)\n",
    "    \n",
    "    duplicates.append(train_DF.is_duplicate[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 387 unique tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunil\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\keras\\preprocessing\\text.py:89: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions1_sequences = tokenizer.texts_to_sequences(questions1)\n",
    "questions2_sequences = tokenizer.texts_to_sequences(questions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions1_sequences = pad_sequences(questions1_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "questions2_sequences = pad_sequences(questions2_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 250)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions2_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (45, 2)\n"
     ]
    }
   ],
   "source": [
    "duplicates_1=duplicates\n",
    "duplicates = to_categorical(np.asarray(duplicates))\n",
    "print('Shape of label tensor:', duplicates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(questions1_sequences.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "questions1_sequences = questions1_sequences[indices]\n",
    "questions2_sequences = questions2_sequences[indices]\n",
    "duplicates = duplicates[indices]\n",
    "\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * questions1_sequences.shape[0])\n",
    "\n",
    "questions1_sequences_train = questions1_sequences[:-nb_validation_samples]\n",
    "questions2_sequences_train = questions2_sequences[:-nb_validation_samples]\n",
    "duplicates_train = duplicates[:-nb_validation_samples]\n",
    "\n",
    "questions1_sequences_val = questions1_sequences[-nb_validation_samples:]\n",
    "questions2_sequences_val = questions2_sequences[-nb_validation_samples:]\n",
    "duplicates_val = duplicates[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing and validation set number of positive and negative reviews\n",
      "[ 26.  10.]\n",
      "[ 6.  3.]\n"
     ]
    }
   ],
   "source": [
    "print('Traing and validation set number of positive and negative reviews')\n",
    "print(duplicates_train.sum(axis=0))\n",
    "print(duplicates_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_symbols = len(word_index) + 1 # adding 1 to account for 0th index (for masking)\n",
    "embedding_weights = np.random.random((n_symbols,EMBEDDING_DIM))\n",
    "for word,index in word_index.items():\n",
    "    if word in model.wv.vocab:\n",
    "        embedding_weights[index] = model.word_vec(word)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(388, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer=Embedding(input_dim=len(word_index)+1,\n",
    "                          output_dim=EMBEDDING_DIM,\n",
    "                          weights=[embedding_weights],\n",
    "                          input_length=MAX_SEQUENCE_LENGTH,\n",
    "                          trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunil\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\keras\\legacy\\layers.py:429: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `sum`, `concatenate`, etc.\n",
      "  warnings.warn('The `merge` function is deprecated '\n",
      "C:\\Users\\sunil\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\keras\\legacy\\layers.py:66: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  warnings.warn('The `Merge` layer is deprecated '\n",
      "C:\\Users\\sunil\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\keras\\legacy\\interfaces.py:86: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[/input_1,..., outputs=sigmoid.0)`\n",
      "  '` call to the Keras 2 API: ' + signature)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model - Siamese Networks\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 250)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 250)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 250, 300)      116400                                       \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 246, 128)      192128                                       \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 246, 128)      192128                                       \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 49, 128)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 49, 128)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 49, 128)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 49, 128)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 6272)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 6272)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 6272)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 2)             12546                                        \n",
      "====================================================================================================\n",
      "Total params: 513,202.0\n",
      "Trainable params: 513,202.0\n",
      "Non-trainable params: 0.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "left_question_input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "right_question_input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "embedded_sequences = embedding_layer(left_question_input)\n",
    "conv1 =Conv1D(128,5,padding='valid',activation='relu')(embedded_sequences)\n",
    "maxpool1=MaxPooling1D(5)(conv1)\n",
    "left_cnn_sent1=Dropout(0.2)(maxpool1)\n",
    "left_cnn_sent=Flatten()(left_cnn_sent1)\n",
    "#Encoders at Question Level\n",
    "\n",
    "embedded_sequences = embedding_layer(right_question_input)\n",
    "conv1 =Conv1D(128,5,padding='valid',activation='relu')(embedded_sequences)\n",
    "maxpool1=MaxPooling1D(5)(conv1)\n",
    "right_cnn_sent1=Dropout(0.2)(maxpool1)\n",
    "right_cnn_sent=Flatten()(right_cnn_sent1)\n",
    "\n",
    "#merge two encoded inputs with the l1 distance between them\n",
    "L1_distance = lambda x: K.abs(x[0]-x[1])\n",
    "both = merge([left_cnn_sent,right_cnn_sent], mode = L1_distance, output_shape=lambda x: x[0])\n",
    "prediction = Dense(2,activation='sigmoid')(both)\n",
    "\n",
    "siamese_net = Model(input=[left_question_input,right_question_input],output=prediction)\n",
    "siamese_net.compile(loss=\"binary_crossentropy\",optimizer=Adam())\n",
    "\n",
    "siamese_net.count_params()\n",
    "\n",
    "print(\"model - Siamese Networks\")\n",
    "print(siamese_net.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_question2_1000_train.shape\n",
    "#duplicates_1000_train=np.array(duplicates_1000_train).reshape(1000,1)\n",
    "#duplicates_1000_vali=np.array(duplicates_1000_vali).reshape(1000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunil\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\keras\\engine\\training.py:1393: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_loss improved from inf to 1.51446, saving model to weights-improvement-00-1.514.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f97ec91710>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_loss:.3f}.hdf5\"\n",
    "checkpoint1 = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint1]\n",
    "# Fit the model\n",
    "siamese_net.fit([questions1_sequences_train,questions2_sequences_train], duplicates_train,\n",
    "                validation_data=([questions1_sequences_val,questions2_sequences_val],duplicates_val)\n",
    "                                  ,nb_epoch=1, batch_size=200,callbacks=callbacks_list,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
